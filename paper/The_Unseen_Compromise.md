# The Unseen Compromise
## When Your Greatest Efficiency Becomes Your Biggest Liability

**AI Security in Practice: Operationalising Risk, Regulation, and Enforceable Controls**  
**Author:** Ali Nouman  
**Audience:** CISOs, security leaders, and risk professionals across UK organisations

---

## Executive Summary: The Compromise We’ve Already Accepted

Here is the truth that sits uncomfortably in every security leader’s Monday morning:  
we have already lost visibility and effective control over how AI is being used in our organisations.

AI is not coming. It is already here, working quietly at employee desks while governance frameworks are debated in committees.

While boards discuss AI strategy in quarterly meetings, millions of UK workers are already using generative AI tools for work, often without formal approval or security oversight.

This is not shadow IT.  
This is **ambient AI**.

It is embedded in tools approved for other purposes, available through platforms organisations already pay for, and woven into the fabric of how work actually gets done.

The gap is not between awareness and action.  
It is between the AI organisations believe they govern and the AI actually being used.

This paper focuses on closing that gap, not through new frameworks, but by extending existing security controls to cover the AI that is already running.

---

## 1. The UK Threat Landscape: When Speed Becomes the Weapon

What is changing across UK organisations is not the nature of cyber threats, but their velocity.

Ransomware has not changed shape, but it has changed speed.  
The UK National Cyber Security Centre continues to handle hundreds of serious incidents annually, with criminal groups operating at industrial scale.

The numbers illustrate the problem clearly:

- **£3.4 million** average cost of a data breach in the UK  
- **207 days** average time to identify a breach  
- **Hours to days** for AI-enhanced attacks to move from access to exfiltration  

Security teams are defending human-paced processes against machine-speed adversaries.  
The margin for error no longer exists.

---

## 2. Ambient AI: The Inventory That Does Not Exist

Consider a simple question.

**How many AI models are processing your organisation’s data right now?**

If the answer is a precise number, it is likely incorrect.  
If the answer is *“I don’t know”*, it is honest and common.

Across UK organisations, security teams are typically aware of **less than 30 percent** of the AI tools actually in use.

A typical mid-sized business has **8 to 12 AI services** processing company data at any given time.

These tools rarely arrive through procurement.  
They appear through:

- Default-enabled SaaS features  
- Browser extensions installed by developers  
- Free-tier accounts created by individuals  
- APIs embedded in automation platforms  

A particularly concerning statistic is that **67 percent of UK professionals admit pasting sensitive company data into public AI tools**.

This behaviour is rarely malicious.  
It is efficient.

And efficiency consistently wins over policy.

---

## 3. Three Realities Security Teams Commonly Miss

### 3.1 Data Leakage by Workflow

In 2023, Samsung engineers reportedly leaked proprietary chip designs via ChatGPT on multiple occasions.

This was not a breach in the traditional sense.  
It was a workflow.

A tool designed to accelerate innovation became the mechanism for intellectual property loss.

Industry analysis predicts that by 2025, the majority of enterprise data will be processed outside traditional security perimeters, not through negligence, but through normal business operations.

---

### 3.2 Prompt Injection as Corporate Espionage

Techniques that bypass AI safety controls are actively traded in underground forums.

These attacks are not classic exploits.  
They are social engineering executed at API scale.

An attacker can embed malicious instructions in publicly accessible content.  
When an internal AI system processes that content, it may execute unintended actions, including data exfiltration.

The attack surface is no longer the firewall.  
It is any text the AI might read.

---

### 3.3 The Supply Chain of Unknowns

When AI vendors experience security incidents, customer organisations may be exposed through no fault of their own.

Research shows that most organisations using third-party AI models have never assessed the underlying model provider’s infrastructure or data handling practices.

As a result, organisations outsource decision-making to systems they do not fully understand and cannot meaningfully audit.

---

## 4. Regulation: The Scaffolding, Not the Building

The EU AI Act will enforce accountability, mandate inventories, and require transparency.

This is necessary, but insufficient.

Regulation defines what must be documented.  
It does not provide operational control.

Security leaders who succeed will use regulation as leverage rather than as a compliance exercise.

In the UK, regulators are already enforcing data protection principles in AI contexts.  
Responsibility applies regardless of which tool processes the data.

---

## 5. The Control Extension Framework

Most organisations already operate mature security controls.

The challenge is extending them to AI use cases.

- Asset management tracks infrastructure, not APIs  
- DLP protects email and files, not prompts  
- Identity systems manage people, not AI agents  
- Vendor risk focuses on suppliers, not models  

AI security becomes manageable when existing controls are **extended**, not replaced.

Prompts must be treated as data exports.  
AI interactions must be logged as API calls.  
Model outputs must be recognised as potential data leaks.

---

## 6. The 90-Day Survival Sprint

This is not a transformation programme.  
It is a visibility and control sprint.

### Days 1–30: See What Is Actually Happening
Analyse outbound traffic to known AI services.  
Run an internal amnesty to understand real usage.  
Publish a simple green, amber, red usage guide.

### Days 31–60: Create Ownership and Reduce Risk
Assign clear ownership.  
Categorise AI use cases by data sensitivity.  
Update vendor contracts with enforceable AI clauses.

### Days 61–90: Make Control Real
Deploy one meaningful technical control.  
Run a realistic tabletop exercise.  
Build a small evidence pack showing actual posture.

The goal is not maturity.  
The goal is control.

---

## 7. Conclusion: The Choice That Defines Your Legacy

AI security is not primarily a technical problem.  
It is a leadership test.

Organisations can choose compliance theatre.  
Or they can choose resilience.

AI is already operating inside organisational boundaries.

The question is not whether AI will be adopted.  
It is whether it will be controlled in practice or only on paper.

Trust will define success in an AI-driven economy.  
Security leadership is how that trust is earned.

---

## References and Sources

- Deloitte UK Generative AI Workforce Study  
  https://www2.deloitte.com/uk/en/pages/consulting/articles/generative-ai-seven-million-workers.html

- UK National Cyber Security Centre Annual Review  
  https://www.ncsc.gov.uk/collection/annual-review

- IBM Cost of a Data Breach Report  
  https://www.ibm.com/reports/data-breach

- IONOS UK AI Workplace Survey  
  https://www.ionos.co.uk/digitalguide/websites/online-marketing/ai-in-the-workplace-uk/

- Gartner AI and Data Security Predictions  
  https://www.gartner.com/en/newsroom/press-releases

- OWASP AI Security and Privacy Guide  
  https://owasp.org/www-project-top-10-for-large-language-model-applications/

- UK Information Commissioner’s Office AI Guidance  
  https://ico.org.uk/for-organisations/ai/
